{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " This notebook is made to show, works LangTokenizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from FuncFiles.convert import *\n",
    "from tqdm.autonotebook import tqdm\n",
    "from FuncFiles.tokenizer import MultigrammTokenizer\n",
    "from FuncFiles.lang_tokenizer import LangTokenizer\n",
    "import codecs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LangTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import file with russian sentences and their translations to English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set of symbols in Russian sentences\n",
      "{'п', '?', 'k', 'ш', 'м', 'щ', 'з', 'd', '5', 'в', '—', 'н', 'f', 'ж', 'б', 'ы', 's', '№', '\"', 'ч', '4', '́', 'r', 'q', 'л', 'n', 'с', 'я', '+', '3', '6', '2', 'х', 'j', '»', 'm', 'ё', 'c', 'к', '9', 'l', ';', 'u', 'y', '1', 'x', 'v', 't', 'т', ':', '8', 'ъ', '.', 'e', 'ю', '–', 'b', 'w', 'a', 'o', '0', 'ц', 'i', 'h', 'p', '«', '!', 'ь', '-', 'р', ' ', 'и', 'г', 'у', 'g', 'й', 'ф', '7', \"'\", 'е', '%', 'э', 'о', ',', 'а', 'д'}\n",
      "86\n",
      "\n",
      "---\n",
      "\n",
      "Set of symbols in English sentences\n",
      "{'8', '?', 'k', 'é', '.', 'e', 'd', '5', '’', 'b', 'w', 'ã', 'a', 'o', '0', 'f', '$', '€', 's', 't', 'i', '‘', 'ü', '\"', 'h', '4', 'p', 'r', '!', 'q', 'n', 'º', '-', '+', ' ', 'z', '3', 'g', '7', '6', \"'\", '2', '%', 'j', 'm', ',', 'c', 'l', '9', ';', 'u', 'y', '1', 'x', 'v', '/', ':'}\n",
      "57\n"
     ]
    }
   ],
   "source": [
    "rus_list_lines = []\n",
    "rus_lines = []\n",
    "eng_list_lines = []\n",
    "eng_lines = []\n",
    "rus_bad_symb = ['\\xa0', '\\u200b']\n",
    "eng_bad_symb = []\n",
    "with codecs.open('rus-eng.txt', encoding='utf-8') as fin:\n",
    "    for line in fin:\n",
    "        rus_curr = line[:-2].lower().split(\"_\")[0]\n",
    "        rus_curr_list = list(rus_curr)\n",
    "        eng_curr = line[:-2].lower().split(\"_\")[1]\n",
    "        eng_curr_list = list(eng_curr)\n",
    "        \n",
    "        for symb in rus_bad_symb:\n",
    "            while symb in rus_curr_list:\n",
    "                rus_curr_list.remove(symb)\n",
    "        rus_lines.append(\"\".join(rus_curr_list))\n",
    "        rus_list_lines.append(rus_curr_list)\n",
    "        \n",
    "        \n",
    "        for symb in eng_bad_symb:\n",
    "            while symb in eng_curr_list:\n",
    "                eng_curr_list.remove(symb)\n",
    "        eng_lines.append(\"\".join(eng_curr_list))\n",
    "        eng_list_lines.append(eng_curr_list)\n",
    "        \n",
    "rus_vocab = set([])\n",
    "eng_vocab = set([])\n",
    "\n",
    "for curr_list in rus_list_lines:\n",
    "    rus_vocab = rus_vocab.union(curr_list)\n",
    "    \n",
    "for curr_list in eng_list_lines:\n",
    "    eng_vocab = eng_vocab.union(curr_list)\n",
    "\n",
    "    \n",
    "print(\"Set of symbols in Russian sentences\")\n",
    "print(rus_vocab)\n",
    "print(len(rus_vocab))\n",
    "print()\n",
    "print(\"---\")\n",
    "print()\n",
    "print(\"Set of symbols in English sentences\")\n",
    "print(eng_vocab)\n",
    "print(len(eng_vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "rus_tokenizer = LangTokenizer(vocab=rus_vocab)\n",
    "eng_tokenizer = LangTokenizer(vocab=eng_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adopt them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db56cf6ba9394bdb9041c23f3fe437c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=2.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "rus_tokenizer.adopt(rus_lines, ad_num=True, num_cycles=2, part_add = 0.50, verbose_main = True, verbose_NGW = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0cb459a9965149f8ad4e680a9e25aedf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=2.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "eng_tokenizer.adopt(eng_lines, ad_num=True, num_cycles=2, part_add = 0.50, verbose_main = True, verbose_NGW = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check most frequently occurring gramms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num:  37580 gramm:  o\n",
      "num:  34670 gramm:  a\n",
      "num:  31676 gramm:  e\n",
      "num:  31504 gramm:  d\n",
      "num:  30855 gramm:  i\n",
      "num:  30491 gramm:  e \n",
      "num:  28887 gramm:  r\n",
      "num:  28692 gramm:  s\n",
      "num:  26357 gramm:  t \n",
      "num:  25924 gramm:  w\n",
      "num:  25215 gramm:  h\n",
      "num:  24496 gramm:   \n",
      "num:  24270 gramm:  c\n",
      "num:  23904 gramm:  m\n",
      "num:  23623 gramm:  k\n",
      "num:  23592 gramm:  p\n",
      "num:  22774 gramm:  l\n",
      "num:  21864 gramm:  g\n",
      "num:  21602 gramm:  t\n",
      "num:  21126 gramm:  u\n",
      "num:  20142 gramm:  n\n",
      "num:  19610 gramm:  y \n",
      "num:  18564 gramm:  d \n",
      "num:  18334 gramm:  f\n",
      "num:  16882 gramm:  i \n",
      "num:  16629 gramm:  you\n",
      "num:  15743 gramm:  b\n",
      "num:  15720 gramm:  re\n",
      "num:  15255 gramm:  er\n",
      "num:  15191 gramm:  tom \n",
      "num:  15061 gramm:   t\n",
      "num:  13899 gramm:  s \n",
      "num:  13864 gramm:  th\n",
      "num:  12798 gramm:  y\n",
      "num:  12645 gramm:  an\n",
      "num:  12541 gramm:  on\n",
      "num:  11682 gramm:   w\n",
      "num:  11517 gramm:  in\n",
      "num:  11192 gramm:  he\n",
      "num:  11181 gramm:  ing\n"
     ]
    }
   ],
   "source": [
    "eng_tokenizer.print_top(40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num:  38464 gramm:  .\n",
      "num:  32425 gramm:  с\n",
      "num:  29887 gramm:  у\n",
      "num:  29142 gramm:  и\n",
      "num:  29102 gramm:   \n",
      "num:  26954 gramm:  т\n",
      "num:  24985 gramm:  е\n",
      "num:  22469 gramm:  я \n",
      "num:  19711 gramm:  а\n",
      "num:  19080 gramm:  н\n",
      "num:  18846 gramm:  м\n",
      "num:  18510 gramm:  ы\n",
      "num:  17875 gramm:  не\n",
      "num:  17850 gramm:  л\n",
      "num:  17752 gramm:  б\n",
      "num:  16634 gramm:  ж\n",
      "num:  16598 gramm:  к\n",
      "num:  16530 gramm:  й\n",
      "num:  16422 gramm:  ш\n",
      "num:  16122 gramm:  в\n",
      "num:  15736 gramm:  на\n",
      "num:  15507 gramm:  з\n",
      "num:  14496 gramm:  том \n",
      "num:  14308 gramm:  ра\n",
      "num:  14237 gramm:  д\n",
      "num:  13781 gramm:  по\n",
      "num:  13023 gramm:  г\n",
      "num:  12989 gramm:   с\n",
      "num:  12718 gramm:  е \n",
      "num:  12696 gramm:  ?\n",
      "num:  12618 gramm:  , \n",
      "num:  12496 gramm:  ю\n",
      "num:  12363 gramm:  ст\n",
      "num:  12125 gramm:  но\n",
      "num:  11580 gramm:  ь \n",
      "num:  11546 gramm:  о \n",
      "num:  11506 gramm:   в\n",
      "num:  10983 gramm:  ч\n",
      "num:  10922 gramm:  о\n",
      "num:  10892 gramm:  ко\n"
     ]
    }
   ],
   "source": [
    "rus_tokenizer.print_top(40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenization and detokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 165, 165, 108, 19, 186, 74, 113, 8, 77, 2]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokened = rus_tokenizer.tokenize(\"мама мыла раму\")\n",
    "tokened"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'мама мыла раму'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rus_tokenizer.detokenize(tokened)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
